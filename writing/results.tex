\section{Evaluations and Discussion}



\subsection{Overhead}




In this section we will measure the latency introduced in the application to record a heartbeat

\subsubsection*{Methodology}



overhead

\subsection{Application}

Andy

\subsection{Experiments}

We will go over three different experiments to evaluate Anchors.

\subsubsection{Experiment 1: Monitoring VMs with Different Schedulers}

In this experiment, we want to evaluate Anchors' ability to monitor and save CPU resource when VMs are running on different subset of cores with different scheduler.

\begin{itemize} 

\item[--] Application:\\
In this experiment, we run the object detection application for on both VM1 and VM2. The duration of each trial is 120 seconds. First 40 seconds, the application is running in high workload mode. The workload is switched to light mode from 40 to 80 seconds. Finally, between 80-120 seconds, the workload is switched to medium mode.




\item[--] VM Configuration: \\
Anchors requires the hypervisor scheduler to assign specific amount of CPU resource to the VMs. With one VM, under RTDS Scheduler, the amount CPU resource allocated to the VM can be specified by assigning proper period and budget. As for Credit Scheduler, which allocates CPU resource based on weights, it is required to create a dummy VM under the same CPU pool so precise CPU resource allocation for the main VM can be achieved by assigning proper weights to the dummy and the main VM. To have a fair comparison, we also create a dummy VM for the CPU pool that runs RTDS Scheduler. Both dummy VMs were assigned 1\% of the CPU utilization before every trial.
The complete VM configuration are shown in table \ref{exp1setup_table}

\begin{table}[ht]
\centering
\caption{My caption}
\label{exp1setup_table}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{}                     & \textbf{\# of VCPUs}     & \textbf{Scheduler}          & \textbf{\begin{tabular}[c]{@{}c@{}}Initial CPU\\ Utilization\end{tabular}} \\ \midrule
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{333333} VM1}    & {\color[HTML]{333333} 5} & {\color[HTML]{333333} RTDS} & {\color[HTML]{333333} 99\%}                                                \\
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{333333} Dummy1} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} RTDS} & {\color[HTML]{333333} 1\%}                                                 \\
\rowcolor[HTML]{C0C0C0} 
VM2                           & 5                        & Credit                      & 99\%                                                                       \\
\rowcolor[HTML]{C0C0C0} 
Dummy2                        & 5                        & Credit                      & 1\%                                                                        \\ \bottomrule
\end{tabular}
\end{table}





\item[--] Methodology:\\
In each trial, after all four VMs are configured as shown in table \ref{exp1setup_table}, VM1 and VM2 runs with application till it is finished. The CPU utilization and FPS data are collected for analysis. The same procedure is repeated with three different resource allocation algorithms: static, AIMD and APID.

\item[--] Result \& Analysis:\\
The results from experiment are shown in figure \ref{1vm_static},\ref{1vm_aimd} and \ref{1vm_apid}


\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/1vm_static}
\caption{Static Algorithm with RTDS/Credit Scheduler}
\label{1vm_static}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/1vm_aimd}
\caption{AIMD Algorithm with RTDS/Credit Scheduler}
\label{1vm_aimd}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/1vm_apid}
\caption{APID Algorithm with RTDS/Credit Scheduler}
\label{1vm_apid}
\end{figure}


\begin{figure*}[t!]
\centering
\begin{subfigure}{.42\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/1vm_rtds_fps}
    \caption{RTDS Scheduler}
    \label{1vm_rtds_fps}
\end{subfigure}
\begin{subfigure}{.42\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/1vm_credit_fps}
    \caption{Credit Scheduler}
    \label{1vm_credit_fps}
\end{subfigure}%
\captionsetup{justification=centering}
\caption{Soft Real-time Performance Comparison}
\label{1vm_rtdscredit_fps}
% \vspace{-0.63cm}
\end{figure*}


\begin{figure*}[t!]
\centering
\begin{subfigure}{.42\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/1vm_rtds_cpu}
    \caption{RTDS Scheduler}
    \label{1vm_rtds_cpu}
\end{subfigure}
\begin{subfigure}{.42\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/1vm_credit_cpu}
    \caption{Credit Scheduler}
    \label{1vm_credit_cpu}
\end{subfigure}%
\captionsetup{justification=centering}
\caption{CPU Savings Comparison}
\label{1vm_rtdscredit_cpu}
% \vspace{-0.63cm}
\end{figure*}




\end{itemize}





\subsubsection{Experiment 2: Monitoring Multiple VMs}

\hfill \linebreak
In this experiment, we use Anchors to monitor VM1 and VM2, which are running on the same set of cores, and show that VM3 is able to benefit from the saved CPU utilization.


\begin{itemize} 
\item[--] Application:\\
There are three VMs for this experiment. VM1 and VM2 run real-time application and VM3 runs non real-time application. 
\begin{itemize} 
\item[o] Random Video Sequence Object Detection: To mimic a real-world video surveillance application, this real-time application read in video streams and if a person is detected in the current frame, the sample frequency is switched from low to medium. If no one is detected in the current frame, the sampling frequency is switched to low frequency. Each trial is run for 120 seconds. The waiting time for a person to show in the frame follows an exponential distribution with $\beta=30$ seconds and the time for a person to stay in the frame follows an exponential distribution with $\beta=15$ seconds. 
\item[o] Matrix Multiplication: a non real-tie application doing loops of 500 by 500 matrix multiplication.
\end{itemize} 


\item[--] VM Configuration:\\
VM1, VM2 and VM3 all run under the same set of PCPU with RTDS Scheduler. VM1 and VM2 are initially assigned with 45\% CPU utilization and VM3 is assigned with 10\% CPU utilization. The configuration is shown in table \ref{exp2setup_table}

\begin{table}[ht]
\centering
\caption{My caption}
\label{exp2setup_table}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{}                     & \textbf{\# of VCPUs}     & \textbf{Scheduler}          & \textbf{\begin{tabular}[c]{@{}c@{}}Initial CPU\\ Utilization\end{tabular}} \\ \midrule
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{333333} VM1}    & {\color[HTML]{333333} 5} & {\color[HTML]{333333} RTDS} & {\color[HTML]{333333} 45\%}                                                \\
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{333333} VM2} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} RTDS} & {\color[HTML]{333333} 45\%}  \\
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{333333} VM3} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} RTDS} & {\color[HTML]{333333} 10\%}                        \\ \bottomrule
\end{tabular}
\end{table}



\item[--] Methodology:\\
First we generate the two random sequences of frames with and without a person for VM1 and VM2 respectively. We run VM1 and VM2 with these two video sequences three times, each time with different a different controller: static, AIMD and APID. FPS and CPU utilizations are recorded for analysis.

As for VM3, during each trial, VM3 starts running matrix multiplication when VM1 or VM2 sends their first heartbeat to Anchors. VM3 stops and reports the numbers of matrix multiplication completed when VM1 and VM1 notify Anchors that their applications are completed.


\item[--] Result \& Analysis:\\

hi meow



\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/3vm_static}
\caption{Static Algorithm with RTDS Scheduler}
\label{3vm_static}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/3vm_aimd}
\caption{AIMD Algorithm with RTDS Scheduler}
\label{3vm_aimd}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/3vm_apid}
\caption{APID Algorithm with RTDS Scheduler}
\label{3vm_apid}
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/3vm_fps}
\caption{Soft Real-time Perforamnce Comparison}
\label{3vm_fps}
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/matmul}
\caption{VM3 Computation Counts Comparison}
\label{matmul}
\end{figure}






\end{itemize}
\subsubsection{Experiment 3: Monitoring VMs in an Overloading System}

In this experiment, we show that even under the worst case scenario, where the system does not have enough CPU resource to satisfy the total demand from the VMs, Anchors is able to improve the overall real-time performance with Stride Scheduling. 


\begin{itemize} 

\item[--] VM Configuration:\\
VM1 and VM2 run under the same set of PCPU with RTDS Scheduler. Both VMs are initially assigned with 50\% CPU utilization. The configuration is shown in table \ref{exp3setup_table}.

\begin{table}[ht]
\centering
\caption{My caption}
\label{exp3setup_table}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{}                     & \textbf{\# of VCPUs}     & \textbf{Scheduler}          & \textbf{\begin{tabular}[c]{@{}c@{}}Initial CPU\\ Utilization\end{tabular}} \\ \midrule
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{333333} VM1}    & {\color[HTML]{333333} 5} & {\color[HTML]{333333} RTDS} & {\color[HTML]{333333} 50\%}                                                \\
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{333333} VM2} & {\color[HTML]{333333} 5} & {\color[HTML]{333333} RTDS} & {\color[HTML]{333333} 50\%}                        \\ \bottomrule
\end{tabular}
\end{table}

\item[--] Application:\\
Both VM1 and VM2 run the same object detection application but with deterministic sequence of person in the frame or not. As shown in figure meow. The reason for using deterministic sequence is because we want to emphasize how Anchors controller handle overloading system. To create a overloading system, medium sampling frequency is used when a person is not in a frame, and high sampling frequency is used when a person is detected in a frame.




\item[--] Methodology:\\
In each trial, after all VM1 and VM2 are configured as shown in table \ref{exp3setup_table}, VM1 and VM2 runs with application till it is finished. The CPU utilization and FPS data are collected for analysis. The same procedure is repeated with three different controllers: static, AIMD and APID.




\item[--] Result \& Analysis:\\

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/2vm_static}
\caption{Static Algorithm with RTDS/Credit Scheduler}
\label{2vm_static}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/2vm_aimd}
\caption{AIMD Algorithm with RTDS/Credit Scheduler}
\label{2vm_aimd}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/2vm_apid}
\caption{APID Algorithm with RTDS/Credit Scheduler}
\label{2vm_apid}
\end{figure}



\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/2vm_fps_static}
\caption{Static Algorithm with RTDS/Credit Scheduler}
\label{2vm_fps_static}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/2vm_fps_aimd}
\caption{AIMD Algorithm with RTDS/Credit Scheduler}
\label{2vm_fps_aimd}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{images/2vm_fps_apid}
\caption{APID Algorithm with RTDS/Credit Scheduler}
\label{2vm_fps_apid}
\end{figure}




\end{itemize}


% \iffalse
% \section{Results and Discussion}
% \label{s4}

% \subsection{Experiment Setup}

% Our simulation adopts the system with seven local fog servers as illustrated in Figure \ref{car7} and an additional cloud server. For fog servers, the link rate between server $i$ and server $j$, $B_{ij}$, is randomly drawn from between 16 and 64 tasks per second for $i\ne j$, or $\infty$ for $i=j$, the CPU frequency for server $j$, ${f_{j}}$, is randomly drawn from between 2700 MHz and 3600 MHz, and the capacity of server $j$, $C_{j}$,  is randomly drawn from between 10 and 35 tasks. For the remote cloud server, its initial tasks, $N_{cloud}$ is 0, $B_{icloud}$ is 4 tasks per second, ${f_{cloud}}$ is 4500MHz, and $C_{cloud}$ is $\infty$. For all servers, CPU cycles required per task, $x$, is set at 35 Mcycles, deadline for each task, $D$, is set at 0.5 second, and allowed processing time for each server, $\tau$, is 0.48 second. The system's parameters can be summarized in Table \ref{simvar}.

% A taskset is generated by randomly selecting a number for $N_{i}$ from a range of integers which is shown in the first row in Table \ref{ntypes}. By varying the range of integers for $N_{i}$, we can create different loadings for the system. In each experiment, for the same type of loading, taskset generation is performed 100 times, and load balancing optimization is executed for each newly generated taskset. We record all the $N_{i}$ values and so each experiment can be run with the same tasksets. All the experiments were run in the machine with dual quadcore AMD Opteron 2.3 GHz with 16GB memory. The time it takes to execute linear prediction and to solve the optimization on that machine are both less than 0.01 second for all the experiments we run. So the overhead for mobility prediction and load balancing is less than 0.02 second, which is within the overhead budget, calculated by substracting deadline by allowed processing time, $D-\tau=0.5-0.48=0.02$.


% \begin{table}[ht!]
% \caption{Simulation Parameters}
% \centering
% \small
% \begin{tabular}{| m{0.8cm} | m{1.6cm} | m{15em} |}
%     \hline
%     \multicolumn{3}{|l|}{\textbf{Simulation Parameters for the System}}\\
%     \hline
%     $k$ & 8 & Number of servers \\
%     \hline
%     $x$ & 35 & Number of CPU cycles required for each task (Mcycles)\\
%     \hline
%     $D$ & 0.5 & Deadline for all tasks (sec)\\
%     \hline
%     $\tau$ & 0.48 & Allowed processing time for each server (sec)\\
%     \hline
%     \multicolumn{3}{|l|}{\textbf{Simulation Parameters for Local Fog Servers}}\\
%     \hline
%     $N_{i}$ & see Table \ref{ntypes} & Number of initial tasks for server $i$\\
%     \hline
%     \multirow{2}{*}{$B_{ij}$} & [16,64] & Link rate between server $i$ and server $j$ for $i\ne j$ (tasks/sec) \\ \cline{2-3}
%     & $\infty$ & Link rate between server $i$ and server $j$ for $i=j$ (tasks/sec) \\
%     % $B_{ij}$ & [16,64] & Link rate between server $i$ and server $j$ (tasks/sec)\\
%     \hline
%     $C_{j}$ & [10,35] & Capacity of server $j$ (tasks)\\
%     \hline
%     $f_{j}$ & [2700,3600] & CPU frequency of server $j$ (MHz)\\
%     \hline
%     \multicolumn{3}{|l|}{\textbf{Simulation Parameters for Cloud Server}}\\
%     \hline
%     $N_{cloud}$ & 0 & Number of inital tasks for cloud server\\
%     \hline
%     $B_{icloud}$ & 4 & Link rate between server $i$ and cloud server (tasks/sec)\\
%     \hline
%     $C_{cloud}$ & $\infty$ & Capacity of cloud server (tasks)\\
%     \hline
%     $f_{cloud}$ & 4500 & CPU frequency of cloud server (MHz)\\
%     \hline\hline
%     \multicolumn{2}{|l|}{\textbf{Optimizing Variables}}\\
%     \hline
%     $n_{ij}$ & solved with optimization & Number of tasks distributed from server $i$ to server $j$\\
%     \hline
% \end{tabular}
% \label{simvar}
% \end{table}

% \begin{table}[h]
% \caption{Types of Workloads}
% \small
% \centering
% \begin{tabular}{|m{1cm}|m{1cm}|m{1cm}|m{1cm}|m{1cm}|m{1cm}|}
%     \hline
%     Range for $N_{i}$ (tasks)& [10,26] & [10,28] & [10,30] & [10,32] & [10,34] \\ \hline
%     Total number of tasks & 12687 & 13472 & 14143 & 14739 & 15051 \\ \hline
% \end{tabular}
% \label{ntypes}
% \end{table}

% \subsection{Deadline Misses vs Total Runtime}

% In this experiment, we focus on the varying $v$, the weighing parameter in the optimization formulation presented. According to our objective function (\ref{opt_fstart}), a greater $v$ value should result in fewer deadline misses with higher total runtime. We verify such trend by running experiment with different $v$ and gather the results for deadline misses and total runtime respectively, as shown in Figure \ref{res_opt_vs_opt_md} and Figure \ref{res_opt_vs_opt_time}. From the figures, we observe that a larger $v$ does result in fewer deadline misses and higher total runtime.


% \begin{figure}[h!]
% \centering
% \includegraphics[width=1\linewidth]{images/res_opt_vs_opt_mdp}
% \caption{Deadline Misses Comparison with Different $v$ }
% \label{res_opt_vs_opt_md}
% \end{figure}


% \begin{figure}[h!]
% \centering
% \includegraphics[width=1\linewidth]{images/res_opt_vs_opt_timep}
% \caption{Total Runtime Comparison with Different $v$ }
% \label{res_opt_vs_opt_time}
% \end{figure}


% \subsection{Comparison with Other Load Balancing Algorithms}



% We compare our optimization result with local static, a simple load balancing method, and three other commonly used load balancing algorithms: weighted round robin, active monitoring and throttled load balancer\cite{wrr}\cite{amt}\cite{amt2}. In local static load balancing, each server with initial tasks exceed its capacity will transfer all the overloading tasks to the cloud server. Weighted round robin is implemented by assigning high weights for local servers that have not reached their maximum capacity. The cloud server will have a higher weight than a local server only when that server reaches its maximum capacity. Active monitoring load balancing is implemented by having each task assigned to the local server that has the highest remaining capacity, the algorithm will only assign to the cloud server when all the local servers are full. In the throttled load balancer, each task is assigned based on the most suitable server available, so the server with lowest completion time and has not reached its maximum capacity will be assigned with the new task. Again, a cloud server will be utilized only when none of the local servers are available. The comparisons are shown in Figure \ref{res_opt_vs_lb_ls_md} and Figure \ref{res_opt_vs_lb_ls_time}. Local static does not utilize any available local server so it is not surprising that local static is outperformed by more than 50\% in every case. On the other hand, our optimization has the least deadline misses and total runtime for every case. The numerical results for deadline misses is presented in Table \ref{good}. From Table \ref{good}, at a lighter load of 13472 tasks, our optimization outperforms throttled load balancer which has the second least deadline misses, by almost 50\%, but as the load of the system increases, the improvement brought by optimization starts to decrease. At the heaviest load of 15051 tasks, the improvment our optimization can bring drops to about 25\%. The reason being a heavy loaded system has fewer available options for task distributing, thus the potential for a better performance gets smaller as more load is added to the system.




 
% % \begin{figure}[ht!]
% % \centering
% % \includegraphics[width=1\linewidth]{images/res_opt_vs_lb_md}
% % \caption{Deadline Misses Comparison with Other Load Balancing Algorithms}
% % \label{res_opt_vs_lb_md}
% % \end{figure}


% % \begin{figure}[ht!]
% % \centering
% % \includegraphics[width=1\linewidth]{images/res_opt_vs_lb_time}
% % \caption{Total Runtime Comparison with Other Load Balancing Algorithms}
% % \label{res_opt_vs_lb_time}
% % \end{figure}

 
% \begin{figure}[ht!]
% \centering
% \includegraphics[width=1\linewidth]{images/res_opt_vs_lb_ls_mdp}
% \caption{Deadline Misses Comparison with Other Load Balancing Algorithms}
% \label{res_opt_vs_lb_ls_md}
% \end{figure}


% \begin{figure}[ht!]
% \centering
% \includegraphics[width=1\linewidth]{images/res_opt_vs_lb_ls_timep}
% \caption{Total Runtime Comparison with Other Load Balancing Algorithms}
% \label{res_opt_vs_lb_ls_time}
% \end{figure}

% \begin{table}[h]
% \small
% \caption{Missed Deadlines Counts}
% \centering
% \begin{tabular}{|m{1.5cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|}
%     \hline
%     \textbf{Total number of tasks} & \textbf{12687} & \textbf{13472} & \textbf{14143} & \textbf{14739} & \textbf{15051} \\ 
%     \hline
%     Optimized, $v$=100 & 0 & 54 & 204 & 389 & 627 \\
%     \hline
%     Local Static & 1052 & 1454 & 2011 & 2300 & 2535 \\
%     \hline
%     Weighted Round Robin & 85 & 240 & 517 & 745& 1003 \\
%     \hline
%     Active Monitoring & 147 & 306 & 606 & 815& 1026 \\
%     \hline
%     Throttled Load Balancer & 11 & 104 & 341 & 568& 837 \\
%     \hline
% \end{tabular}
% \label{good}
% \end{table}

% % \begin{table}[h]
% % \small
% % \caption{Missed Deadlines Counts}
% % \centering
% % \begin{tabular}{|m{1.5cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|}
% %     \hline
% %     \textbf{Total number of tasks} & \textbf{12687} & \textbf{13472} & \textbf{14143} & \textbf{14739} & \textbf{15051} \\ 
% %     \hline
% %     Optimized, $v$=100 & 0 & 54 & 204 & 389 & 627 \\
% %     \hline
% %     Local Static & 1052 & 1454 & 2011 & 2300 & 2535 \\
% %     \hline
% %     Weighted Round Robin & 85 & 240 & 517 & 745& 1003 \\
% %     \hline
% %     Active Monitoring & 147 & 306 & 606 & 815& 1026 \\
% %     \hline
% %     Throttled Load Balancer & 11 & 104 & 341 & 568& 837 \\
% %     \hline
% % \end{tabular}
% % \label{good}
% % \end{table}


% % \begin{table}[h]
% % \small
% % \caption{Deadline Misses ratio - deadline misses/total tasks, $v$=100}
% % \centering
% % \begin{tabular}{|m{1.5cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|}
% %     \hline
% %     \textbf{Total number of tasks} & \textbf{12687} & \textbf{13472} & \textbf{14143} & \textbf{14739} & \textbf{15051} \\  
% %     \hline
% %     Optimized, $v$=100 & 0\% & .40\% & 1.44\% & 2.64\% & 4.17\% \\
% %     \hline
% %     Local Static & 8.29\% & 10.79\% & 14.2\% & 15.6\% & 16.8\% \\
% %     \hline
% %     Weighted Roung Robin & 0.67\% & 1.78\% & 3.66\% & 5.05\% & 6.66\% \\
% %     \hline
% %     Active Monitoring  & 1.16\% & 2.27\% & 4.28\% & 5.53\% & 6.82\% \\    
% %     \hline
% %     Thorttled Load Balancer & 0.087\% & 0.77\% & 2.41\% & 3.85\% & 5.56\% \\
% %     \hline
% % \end{tabular}
% % \label{bad}
% % \end{table}



% % \begin{table}[h]
% % \small
% % \caption{Deadline Misses Comparison to Optimized \%, $v$=100}
% % \centering
% % \begin{tabular}{|m{1.5cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|m{0.75cm}|}
% %     \hline
% %     \textbf{Total number of tasks} & \textbf{12687} & \textbf{13472} & \textbf{14143} & \textbf{14739} & \textbf{15051} \\ 
% %     \hline
% %     Local Static & 100\% & 96.3\% & 89.9\% & 83.1\% & 75.3\% \\
% %     \hline
% %     Weighted Roung Robin  & 100\% & 77.5\% & 60.5\% & 48.8\%& 37.5\% \\
% %     \hline
% %     Active Monitoring  & 100\% & 82.4\% & 66.3\% & 52.3\%& 38.9\% \\
% %     \hline
% %     Thorttled Load Balancer & 100\% & 48.1\% & 40.2\% & 31.5\%& 25.1\% \\
% %     \hline
% % \end{tabular}
% % \label{best}
% % \end{table}



% \fi












